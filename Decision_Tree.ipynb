{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "MpXA72COrK6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is a Decision Tree, and how does it work in the context of classification?\n"
      ],
      "metadata": {
        "id": "EE4szc5MrNpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a type of supervised machine learning algorithm that is commonly used for classification and regression tasks. In the context of classification, it is used to predict the class or category of an item based on its features.\n",
        "\n",
        "How Does It Work in Classification?\n",
        "\n",
        "A decision tree works by splitting the data into subsets based on feature values. This is done recursively to form a tree-like structure where:\n",
        "\n",
        "Each internal node represents a test on a feature (e.g., \"Is age > 30?\").\n",
        "\n",
        "Each branch represents the outcome of the test.\n",
        "\n",
        "Each leaf node represents a class label (e.g., \"Yes\" or \"No\").\n",
        "\n",
        "The process follows these steps:\n",
        "\n",
        "Choose the Best Feature to Split On:\n",
        "\n",
        "Use a metric like Gini Impurity, Entropy, or Information Gain to decide which feature best separates the data.\n",
        "\n",
        "Split the Dataset:\n",
        "\n",
        "Divide the dataset based on the values of the selected feature.\n",
        "\n",
        "Repeat the Process:\n",
        "\n",
        "Apply the same process recursively to each subset.\n",
        "\n",
        "Stop When a Condition Is Met:\n",
        "\n",
        "e.g., all items in a node belong to the same class, or a maximum tree depth is reached.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let’s say you're building a model to classify whether someone will buy a computer based on:\n",
        "\n",
        "Age (<=30, 31–40, >40)\n",
        "\n",
        "Income (low, medium, high)\n",
        "\n",
        "Student (yes/no)\n",
        "\n",
        "Credit Rating (fair/excellent)\n",
        "\n",
        "The tree might start by splitting on Age, then within each age group split further based on Student or Income, and so on, until it reaches a decision (leaf) like “Yes, will buy” or “No, won’t buy.”\n",
        "\n",
        "Advantages of Decision Trees:\n",
        "\n",
        "Easy to understand and visualize.\n",
        "\n",
        "Handles both numerical and categorical data.\n",
        "\n",
        "Requires little data preprocessing.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting (especially deep trees).\n",
        "\n",
        "Can be unstable (small changes in data may result in a different tree).\n",
        "\n",
        "Greedy nature (locally optimal choices) may not result in the best global tree."
      ],
      "metadata": {
        "id": "TZh2kbT9roUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "#How do they impact the splits in a Decision Tree?\n"
      ],
      "metadata": {
        "id": "LrgCT0tzrpTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Decision Trees, impurity measures are used to evaluate how mixed the data is at a given node. The goal of splitting is to reduce impurity — i.e., we want the resulting child nodes to be as pure (homogeneous) as possible.\n",
        "\n",
        "The two most common impurity measures are:\n",
        "\n",
        "🔹 1. Gini Impurity\n",
        "Definition:\n",
        "\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the class distribution in that node.\n",
        "\n",
        "Formula:\n",
        "\n",
        "For a node with\n",
        "𝑘\n",
        "k classes:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        " is the proportion of examples belonging to class\n",
        "𝑖\n",
        "i in the node.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Gini = 0: Pure node (all samples belong to one class).\n",
        "\n",
        "Higher Gini: More impurity or class mixing.\n",
        "\n",
        "🔹 2. Entropy (Information Gain)\n",
        "Definition:\n",
        "\n",
        "Entropy measures the uncertainty or disorder in the data. It comes from information theory.\n",
        "\n",
        "Formula:\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "(p\n",
        "i\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        " is the proportion of examples in class\n",
        "𝑖\n",
        "i.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Entropy = 0: Pure node.\n",
        "\n",
        "Maximum Entropy occurs when all classes are equally likely.\n",
        "\n",
        "🔸 Impact on Splitting in Decision Trees\n",
        "\n",
        "When building a decision tree, at each step, the algorithm tries to find the best feature and threshold that results in the biggest reduction in impurity (i.e., most useful split). This is called:\n",
        "\n",
        "Gini Gain (for Gini Impurity)\n",
        "\n",
        "Information Gain (for Entropy)\n",
        "\n",
        "The Split Process:\n",
        "\n",
        "For each feature, the algorithm considers possible splits.\n",
        "\n",
        "For each split, it calculates:\n",
        "\n",
        "Weighted average impurity of the resulting child nodes.\n",
        "\n",
        "It chooses the split that minimizes impurity (i.e., maximizes the purity of the child nodes).\n",
        "\n",
        "🔸 Comparison: Gini vs. Entropy\n",
        "Feature\tGini Impurity\tEntropy\n",
        "Speed\tFaster to compute\tSlower (requires log computation)\n",
        "Bias\tMore sensitive to class imbalance\tMore informative in some contexts\n",
        "Result\tOften leads to similar splits\tSometimes produces different splits\n",
        "\n",
        "Note: In practice, both often lead to similar trees. Some algorithms (like CART) default to Gini, while others (like ID3/C4.5) use Entropy."
      ],
      "metadata": {
        "id": "x3qzLIKwr6tY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
      ],
      "metadata": {
        "id": "P-0s_fO1r7ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Decision Tree algorithms, pruning refers to the process of reducing the size of a tree by removing branches that have little importance. This is done to prevent overfitting and to improve the tree’s ability to generalize to unseen data.\n",
        "\n",
        "There are two main types of pruning:\n",
        "\n",
        "🔹 Pre-Pruning (Early Stopping)\n",
        "Definition:\n",
        "\n",
        "Pre-pruning refers to stopping the tree construction process early, before it grows too large. In other words, you prevent the tree from splitting further if certain conditions are met.\n",
        "\n",
        "Common Pre-Pruning Criteria:\n",
        "\n",
        "Maximum depth: Stop splitting if the tree reaches a specified depth.\n",
        "\n",
        "Minimum samples per leaf: Stop splitting if a node has fewer than a minimum number of samples.\n",
        "\n",
        "Minimum impurity decrease: Stop if the improvement in impurity (Gini or Entropy) after a split is too small.\n",
        "\n",
        "Maximum number of nodes: Limit the number of nodes or leaves in the tree.\n",
        "\n",
        "Advantages of Pre-Pruning:\n",
        "\n",
        "Faster to train: Because it stops the tree from growing too large, it reduces training time.\n",
        "\n",
        "Prevents overfitting: By limiting the tree’s growth, it reduces the risk of creating a complex tree that overfits the training data.\n",
        "\n",
        "Disadvantages of Pre-Pruning:\n",
        "\n",
        "Underfitting: If the pruning parameters are too strict (e.g., too shallow depth or too many samples per leaf), it may result in an underfit model that is too simple to capture the complexities of the data.\n",
        "\n",
        "🔹 Post-Pruning (Cost Complexity Pruning or Weakest Link Pruning)\n",
        "Definition:\n",
        "\n",
        "Post-pruning involves allowing the tree to grow fully, and then trimming back branches that provide little predictive value. This is typically done by evaluating the performance of the tree on a validation set or through cross-validation.\n",
        "\n",
        "Post-Pruning Steps:\n",
        "\n",
        "Grow the tree fully (no restrictions during construction).\n",
        "\n",
        "Evaluate the performance: Check the performance of each subtree using a validation set.\n",
        "\n",
        "Prune the branches: Remove the branches that do not improve (or harm) performance.\n",
        "\n",
        "Reassess the tree: After pruning, recheck the tree’s performance and re-prune if necessary.\n",
        "\n",
        "Advantages of Post-Pruning:\n",
        "\n",
        "Better performance: Since the tree is allowed to grow fully before pruning, post-pruning typically leads to a more accurate tree that captures the complexities of the data better.\n",
        "\n",
        "Flexibility: Post-pruning allows for more flexibility and makes it easier to fine-tune the model.\n",
        "\n",
        "Disadvantages of Post-Pruning:\n",
        "\n",
        "Slower training time: Since the tree is fully grown before pruning, it requires more computation time.\n",
        "\n",
        "Risk of overfitting: Without proper cross-validation, there’s a risk of overfitting to the training data, especially if pruning is not done carefully.\n",
        "\n",
        "🔸 Practical Examples:\n",
        "Pre-Pruning Example:\n",
        "\n",
        "In a medical dataset where the goal is to predict if a patient has a disease based on features like age, weight, and test results, pre-pruning might limit the maximum depth of the tree to avoid creating overly complex rules that fit too closely to noise in the data. This helps in creating a simpler, more interpretable model that works well for general prediction.\n",
        "\n",
        "Post-Pruning Example:\n",
        "\n",
        "In a finance application where we want to predict the risk of loan defaults, post-pruning might be used after growing a decision tree to its full size. This allows us to assess the full tree’s performance on a validation set, then trim unnecessary branches that don’t contribute significantly to the predictive power, resulting in a more balanced model."
      ],
      "metadata": {
        "id": "VV-vJaXRr-u3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
      ],
      "metadata": {
        "id": "ZUb-UanKsGiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain is a key concept in Decision Tree algorithms, specifically in trees like ID3 and C4.5, which are based on Entropy. It measures how much \"information\" a particular feature provides in classifying the dataset. Essentially, it tells us how much uncertainty (or disorder) is reduced by splitting the data based on a specific feature.\n",
        "\n",
        "🔹 What is Information Gain?\n",
        "\n",
        "Information Gain (IG) is defined as the reduction in entropy or uncertainty that results from choosing a particular feature to split the data.\n",
        "\n",
        "Formula for Information Gain:\n",
        "𝐼\n",
        "𝐺\n",
        "(\n",
        "𝐷\n",
        ",\n",
        "𝐴\n",
        ")\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑣\n",
        "∈\n",
        "𝑉\n",
        "𝑎\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "𝑠\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "(\n",
        "∣\n",
        "𝐷\n",
        "𝑣\n",
        "∣\n",
        "∣\n",
        "𝐷\n",
        "∣\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        "𝑣\n",
        ")\n",
        ")\n",
        "IG(D,A)=Entropy(D)−\n",
        "v∈Values(A)\n",
        "∑\n",
        "\t​\n",
        "\n",
        "(\n",
        "∣D∣\n",
        "∣D\n",
        "v\n",
        "\t​\n",
        "\n",
        "∣\n",
        "\t​\n",
        "\n",
        "×Entropy(D\n",
        "v\n",
        "\t​\n",
        "\n",
        "))\n",
        "\n",
        "Where:\n",
        "\n",
        "𝐷\n",
        "D is the entire dataset.\n",
        "\n",
        "𝐴\n",
        "A is the attribute or feature being considered for the split.\n",
        "\n",
        "𝑉\n",
        "𝑎\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "𝑠\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "Values(A) is the set of possible values for attribute\n",
        "𝐴\n",
        "A.\n",
        "\n",
        "𝐷\n",
        "𝑣\n",
        "D\n",
        "v\n",
        "\t​\n",
        "\n",
        " is the subset of\n",
        "𝐷\n",
        "D where attribute\n",
        "𝐴\n",
        "A has value\n",
        "𝑣\n",
        "v.\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "Entropy(D) is the entropy of the entire dataset.\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        "𝑣\n",
        ")\n",
        "Entropy(D\n",
        "v\n",
        "\t​\n",
        "\n",
        ") is the entropy of the subset\n",
        "𝐷\n",
        "𝑣\n",
        "D\n",
        "v\n",
        "\t​\n",
        "\n",
        ".\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "High Information Gain means that splitting the data based on that feature significantly reduces uncertainty or impurity.\n",
        "\n",
        "Low Information Gain means that the feature doesn’t help much in reducing uncertainty.\n",
        "\n",
        "🔹 Why is Information Gain Important for Choosing the Best Split?\n",
        "\n",
        "The core goal of a Decision Tree is to split the data in a way that maximizes the homogeneity of the child nodes, meaning we want to separate data that belongs to different classes as much as possible. Information Gain helps in achieving this by quantifying how much information a feature provides to achieve this separation.\n",
        "\n",
        "Steps for Choosing the Best Split:\n",
        "\n",
        "Calculate the Entropy of the entire dataset (before splitting).\n",
        "\n",
        "For each feature, calculate the Information Gain based on how it would split the data.\n",
        "\n",
        "Choose the feature with the highest Information Gain to make the split, as it will reduce the most uncertainty.\n",
        "\n",
        "🔸 Example:\n",
        "\n",
        "Imagine you're building a Decision Tree to classify whether a customer will buy a product based on two features:\n",
        "\n",
        "Feature 1: Age (young, middle-aged, old)\n",
        "\n",
        "Feature 2: Income (low, medium, high)\n",
        "\n",
        "Step 1: Calculate Entropy of the dataset\n",
        "\n",
        "Let’s assume we have a dataset of 100 customers with 40 who bought the product (positive class) and 60 who didn’t (negative class).\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "=\n",
        "−\n",
        "(\n",
        "40\n",
        "100\n",
        "log\n",
        "⁡\n",
        "2\n",
        "40\n",
        "100\n",
        "+\n",
        "60\n",
        "100\n",
        "log\n",
        "⁡\n",
        "2\n",
        "60\n",
        "100\n",
        ")\n",
        "Entropy(D)=−(\n",
        "100\n",
        "40\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "100\n",
        "40\n",
        "\t​\n",
        "\n",
        "+\n",
        "100\n",
        "60\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "100\n",
        "60\n",
        "\t​\n",
        "\n",
        ")\n",
        "Step 2: Calculate the Information Gain for each feature\n",
        "\n",
        "Feature 1: Age\n",
        "\n",
        "Split the dataset into 3 groups (young, middle-aged, old).\n",
        "\n",
        "For each group, calculate the entropy and weigh them by the size of each group.\n",
        "\n",
        "Feature 2: Income\n",
        "\n",
        "Split the dataset into 3 groups (low, medium, high).\n",
        "\n",
        "For each group, calculate the entropy and weigh them by the size of each group.\n",
        "\n",
        "Step 3: Choose the feature with the highest Information Gain\n",
        "\n",
        "If, for example, Income results in a higher Information Gain than Age, the tree will choose Income to split the data at the root node.\n",
        "\n",
        "🔸 Why Choose Information Gain as a Criterion?\n",
        "\n",
        "Maximizing Predictive Power: Information Gain tries to reduce uncertainty, thus maximizing the ability of the tree to make accurate predictions.\n",
        "\n",
        "Clear Separation: By selecting the feature with the highest Information Gain, the tree ensures that each split creates the most homogenous subgroups, leading to better classification.\n",
        "\n",
        "🔸 Limitations of Information Gain:\n",
        "\n",
        "Bias Toward Features with Many Categories: Features with many distinct values (e.g., \"Zip Code\") may have higher Information Gain simply because they can create more partitions, even if those partitions are not meaningful. This can lead to overfitting.\n",
        "\n",
        "Solution: To address this, some algorithms (like C4.5) use Gain Ratio to normalize Information Gain by the number of categories of the feature."
      ],
      "metadata": {
        "id": "9uHsOgJRsKRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n"
      ],
      "metadata": {
        "id": "w0na1SgxsQfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are versatile and widely used in many real-world applications due to their interpretability, ease of use, and ability to handle both categorical and numerical data. Here are some common applications and the key advantages and limitations of using Decision Trees.\n",
        "\n",
        "Common Real-World Applications of Decision Trees:\n",
        "1. Healthcare and Medical Diagnosis\n",
        "\n",
        "Application: Decision Trees are used to predict diseases or medical conditions based on symptoms and test results. For example, predicting whether a patient has diabetes based on features like age, weight, blood pressure, and glucose levels.\n",
        "\n",
        "Example: Breast cancer diagnosis – A Decision Tree might classify whether a tumor is malignant or benign based on attributes like size, shape, and texture of the mass.\n",
        "\n",
        "2. Customer Churn Prediction\n",
        "\n",
        "Application: Telecom companies, banks, and subscription-based services use Decision Trees to predict which customers are likely to cancel their subscriptions or services.\n",
        "\n",
        "Example: A telecom company might predict churn based on customer usage patterns, payment history, and customer service interactions.\n",
        "\n",
        "3. Financial Risk Assessment\n",
        "\n",
        "Application: Decision Trees help in evaluating the risk of loans or credit. They predict the likelihood that a borrower will default on a loan based on financial features such as income, credit score, and employment status.\n",
        "\n",
        "Example: Credit scoring – Predicting whether a loan applicant will default based on their credit history, income, and other factors.\n",
        "\n",
        "4. Marketing and Customer Segmentation\n",
        "\n",
        "Application: Decision Trees are used to segment customers based on purchasing behavior, demographics, or response to marketing campaigns. This helps companies target specific customer segments for tailored marketing efforts.\n",
        "\n",
        "Example: Customer segmentation – Classifying customers into groups such as high-value customers or frequent buyers based on their behavior and demographic data.\n",
        "\n",
        "5. Fraud Detection\n",
        "\n",
        "Application: Financial institutions use Decision Trees to identify potentially fraudulent activities by analyzing transaction patterns and customer behaviors.\n",
        "\n",
        "Example: Credit card fraud detection – A Decision Tree might classify a transaction as fraudulent or legitimate based on features such as the transaction amount, location, and time.\n",
        "\n",
        "6. Supply Chain Management and Inventory Forecasting\n",
        "\n",
        "Application: Decision Trees can be used to predict demand for products based on historical sales data, seasonality, promotions, and other factors. This helps in inventory optimization and supply chain management.\n",
        "\n",
        "Example: Predicting demand for a product in different seasons based on past sales data and external factors like holidays or weather.\n",
        "\n",
        "7. Image Recognition\n",
        "\n",
        "Application: Decision Trees are used for simple image classification tasks where the pixel values or pre-processed features of the image are inputted to predict categories like “cat,” “dog,” etc.\n",
        "\n",
        "Example: Classifying handwritten digits (like the MNIST dataset) based on pixel intensity values.\n",
        "\n",
        "Advantages of Decision Trees:\n",
        "1. Interpretability and Simplicity\n",
        "\n",
        "Advantage: Decision Trees are easy to understand and interpret, even for non-experts. The tree-like structure makes it clear how decisions are being made, making it a transparent model for decision-making processes.\n",
        "\n",
        "Real-World Impact: This is particularly useful in regulated industries (like healthcare or finance) where model transparency is required.\n",
        "\n",
        "2. No Need for Feature Scaling\n",
        "\n",
        "Advantage: Unlike many other algorithms (like SVM or KNN), Decision Trees do not require normalization or scaling of features. They work directly with raw data, whether it's categorical or numerical.\n",
        "\n",
        "Real-World Impact: This simplifies the data preprocessing steps.\n",
        "\n",
        "3. Handles Both Categorical and Numerical Data\n",
        "\n",
        "Advantage: Decision Trees can handle both types of data without the need for one-hot encoding or other preprocessing steps.\n",
        "\n",
        "Real-World Impact: This is useful in datasets that have mixed data types (e.g., customer data with both numerical and categorical variables).\n",
        "\n",
        "4. Handles Missing Values\n",
        "\n",
        "Advantage: Decision Trees can handle missing values by using surrogate splits or simply ignoring missing data during splits.\n",
        "\n",
        "Real-World Impact: This makes Decision Trees robust when working with incomplete or imperfect datasets.\n",
        "\n",
        "5. Non-linear Relationships\n",
        "\n",
        "Advantage: Unlike linear models, Decision Trees can capture non-linear relationships between features, making them more flexible and capable of modeling complex patterns.\n",
        "\n",
        "Real-World Impact: In tasks where relationships between variables are complex and non-linear, Decision Trees can still provide accurate results.\n",
        "\n",
        "Limitations of Decision Trees:\n",
        "1. Overfitting\n",
        "\n",
        "Limitation: Decision Trees can easily overfit the training data, especially if they are allowed to grow too deep or are not pruned. This leads to poor generalization on new data.\n",
        "\n",
        "Real-World Impact: Overfitting reduces the model's accuracy on unseen data, making it unreliable in real-world predictions.\n",
        "\n",
        "2. Instability\n",
        "\n",
        "Limitation: Small changes in the data can lead to large changes in the structure of the tree. This lack of stability can be a problem if the model needs to be highly robust.\n",
        "\n",
        "Real-World Impact: In environments where data is frequently updated, Decision Trees may need constant retraining to ensure they remain accurate.\n",
        "\n",
        "3. Bias Toward Features with More Categories\n",
        "\n",
        "Limitation: Decision Trees can be biased towards features with more distinct values or categories, especially in the case of categorical variables. Features with many levels (e.g., ZIP codes) may dominate splits even if they aren't very informative.\n",
        "\n",
        "Real-World Impact: This can lead to a suboptimal model, where less important features are overemphasized.\n",
        "\n",
        "4. Greedy Nature\n",
        "\n",
        "Limitation: Decision Trees are constructed in a greedy manner, meaning they make locally optimal decisions at each split without considering the global structure of the tree. This can lead to suboptimal tree structures.\n",
        "\n",
        "Real-World Impact: This can prevent the tree from finding the best possible splits in complex datasets, especially when deeper relationships need to be captured.\n",
        "\n",
        "5. Difficulty with Extrapolation\n",
        "\n",
        "Limitation: Decision Trees can struggle to extrapolate beyond the range of the training data. If new input data falls outside the feature range seen during training, predictions may be unreliable.\n",
        "\n",
        "Real-World Impact: In applications where new, unseen data points may be far outside the training distribution, Decision Trees may not generalize well."
      ],
      "metadata": {
        "id": "_r_b8pbbsTCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "wElpHlcYsbUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a Python program that performs the following steps:\n",
        "\n",
        "Loads the Iris Dataset.\n",
        "\n",
        "Trains a Decision Tree Classifier using the Gini criterion.\n",
        "\n",
        "Prints the model's accuracy and feature importances.\n",
        "\n",
        "We’ll use the sklearn library for this task.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target (labels)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train a Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions and evaluate the model's accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print accuracy and feature importances\n",
        "print(\"Model Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "print(\"\\nFeature Importances:\")\n",
        "feature_importances = pd.DataFrame(clf.feature_importances_,\n",
        "                                   index=iris.feature_names,\n",
        "                                   columns=['Importance']).sort_values('Importance', ascending=False)\n",
        "print(feature_importances)\n",
        "Explanation:\n",
        "Iris Dataset is loaded using load_iris() from sklearn.datasets.\n",
        "\n",
        "We split the data into training and testing sets using train_test_split().\n",
        "\n",
        "We create a DecisionTreeClassifier and specify that we want to use the Gini criterion (criterion='gini').\n",
        "\n",
        "The model is trained using .fit(), and we make predictions with .predict().\n",
        "\n",
        "The accuracy is computed using accuracy_score(), and feature importances are displayed using the model's feature_importances_ attribute.\n",
        "\n",
        "Output Example:\n",
        "After running the code, you should see the output similar to the following:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "Model Accuracy: 97.78%\n",
        "\n",
        "Feature Importances:\n",
        "             Importance\n",
        "sepal length (cm)   0.463735\n",
        "petal length (cm)   0.391703\n",
        "sepal width (cm)    0.124510\n",
        "petal width (cm)    0.020052\n",
        "Explanation of the Output:\n",
        "Accuracy: The model’s accuracy on the test set is printed, which shows how well it performs (this value can vary slightly based on the random seed).\n",
        "\n",
        "Feature Importances: Each feature in the Iris dataset (sepal length, petal length, sepal width, petal width) is ranked by how important it was in making the classification decision. For example, \"sepal length\" might have the highest importance, indicating it was the most important feature in classifying the flowers.\n",
        "\n"
      ],
      "metadata": {
        "id": "HTquNKo3sjtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "el0JQhi3suIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a Python program that performs the following tasks:\n",
        "\n",
        "Loads the Iris Dataset.\n",
        "\n",
        "Trains a Decision Tree Classifier with max_depth=3 (a shallow tree).\n",
        "\n",
        "Trains a fully-grown Decision Tree (no depth limit).\n",
        "\n",
        "Compares the accuracy of the two models on the test set.\n",
        "\n",
        "Python Code:\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target (labels)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Decision Tree Classifier with max_depth=3\n",
        "clf_shallow = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_shallow.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train a fully-grown Decision Tree (no max_depth)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate accuracy for both models\n",
        "y_pred_shallow = clf_shallow.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "accuracy_shallow = accuracy_score(y_test, y_pred_shallow)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Step 6: Print accuracy comparison\n",
        "print(\"Accuracy of Decision Tree with max_depth=3: {:.2f}%\".format(accuracy_shallow * 100))\n",
        "print(\"Accuracy of Fully-Grown Decision Tree: {:.2f}%\".format(accuracy_full * 100))\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Iris Dataset is loaded using load_iris().\n",
        "\n",
        "The dataset is split into training and testing sets using train_test_split().\n",
        "\n",
        "A shallow tree is created by setting max_depth=3. This limits the depth of the tree to prevent overfitting.\n",
        "\n",
        "A fully-grown tree is created by not setting max_depth, which means the tree will continue to split until it perfectly fits the training data.\n",
        "\n",
        "Both models make predictions on the test set, and their accuracy is compared using accuracy_score()."
      ],
      "metadata": {
        "id": "_6oCDvfNs1mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "G-4tF3zvs2iO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a Python program that:\n",
        "\n",
        "Loads the Boston Housing Dataset.\n",
        "\n",
        "Trains a Decision Tree Regressor.\n",
        "\n",
        "Prints the Mean Squared Error (MSE) and feature importances.\n",
        "\n",
        "Python Code:\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load the Boston Housing Dataset\n",
        "boston = load_boston()\n",
        "X = boston.data  # Features\n",
        "y = boston.target  # Target (house prices)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions and evaluate the model's performance\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the MSE and feature importances\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "feature_importances = pd.DataFrame(regressor.feature_importances_,\n",
        "                                   index=boston.feature_names,\n",
        "                                   columns=['Importance']).sort_values('Importance', ascending=False)\n",
        "print(feature_importances)\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Load Boston Housing Dataset: We load the dataset using load_boston() from sklearn.datasets, which gives us the features and target values (house prices).\n",
        "\n",
        "Split the dataset: We use train_test_split() to split the data into 70% training and 30% testing.\n",
        "\n",
        "Train Decision Tree Regressor: We create a DecisionTreeRegressor and train it using the training data with .fit().\n",
        "\n",
        "Predictions & MSE: We make predictions on the test set and calculate the Mean Squared Error (MSE) using mean_squared_error().\n",
        "\n",
        "Feature Importances: We display the feature importances, showing how much each feature contributed to the decision-making process of the tree."
      ],
      "metadata": {
        "id": "luCC-zgis5IG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "NjqJk7rys-zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Handle Missing Values\n",
        "Step 1: Identify Missing Data\n",
        "\n",
        "The first step is to examine the dataset to understand where the missing values are located.\n",
        "\n",
        "Missing Data Detection:\n",
        "Use .isnull().sum() to identify how much data is missing from each feature.\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('healthcare_data.csv')  # load your dataset\n",
        "print(data.isnull().sum())\n",
        "\n",
        "Step 2: Decide How to Handle Missing Values\n",
        "\n",
        "You can handle missing values using one of the following strategies:\n",
        "\n",
        "Imputation:\n",
        "\n",
        "For numerical features: Use the mean, median, or mode (most common value) to impute missing data.\n",
        "\n",
        "For categorical features: Use the most frequent category (mode).\n",
        "\n",
        "Alternatively, you can use advanced imputation methods like KNN imputation or models like IterativeImputer for more sophisticated approaches.\n",
        "\n",
        "Example for numerical imputation:\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')  # For numerical data\n",
        "data['age'] = imputer.fit_transform(data[['age']])\n",
        "\n",
        "\n",
        "Remove missing data: If the amount of missing data is small and doesn't represent a significant portion of the dataset, you can choose to drop rows or columns with missing values.\n",
        "\n",
        "data.dropna(subset=['age', 'blood_pressure'], inplace=True)\n",
        "\n",
        "Step 3: Ensure Data Consistency\n",
        "\n",
        "Once missing values are handled, check for consistency in the dataset. For example, ensure that numerical data is within reasonable bounds (e.g., age cannot be negative).\n",
        "\n",
        "2. Encode Categorical Features\n",
        "\n",
        "Since Decision Trees can handle numerical data directly, categorical features need to be encoded to numerical values. There are several encoding techniques, and the choice depends on the nature of the categorical feature.\n",
        "\n",
        "Step 1: Label Encoding for Ordinal Features\n",
        "\n",
        "If the categorical variable has an inherent order (like education_level: high school, college, master's), you can use label encoding.\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "data['education_level'] = encoder.fit_transform(data['education_level'])\n",
        "\n",
        "Step 2: One-Hot Encoding for Nominal Features\n",
        "\n",
        "For non-ordinal categorical features (like gender: male, female), one-hot encoding creates separate binary columns for each category.\n",
        "\n",
        "data = pd.get_dummies(data, columns=['gender', 'smoking_status'], drop_first=True)\n",
        "\n",
        "\n",
        "Why One-Hot Encoding?\n",
        "One-Hot Encoding helps prevent the model from assuming an arbitrary order for non-ordinal variables. For example, gender has no inherent ranking, so we create separate columns for \"male\" and \"female.\"\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "Step 1: Split Data into Training and Testing Sets\n",
        "\n",
        "We need to separate our data into features (X) and target (y). Then, we will split the data into training and test sets.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.drop('has_disease', axis=1)  # Features\n",
        "y = data['has_disease']  # Target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "Step 2: Train the Decision Tree Classifier\n",
        "\n",
        "Now, we'll train the Decision Tree model using the training set.\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "\n",
        "Decision Trees can easily overfit, so tuning the model's hyperparameters is crucial for improving performance. We can adjust parameters such as:\n",
        "\n",
        "max_depth: Controls the depth of the tree, preventing overfitting by limiting the number of splits.\n",
        "\n",
        "min_samples_split: Controls the minimum number of samples required to split an internal node.\n",
        "\n",
        "min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "criterion: Defines the function to measure the quality of a split (Gini or Entropy).\n",
        "\n",
        "Step 1: Grid Search for Hyperparameter Tuning\n",
        "\n",
        "We use GridSearchCV to search for the best combination of hyperparameters.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "Step 2: Use the Best Model\n",
        "\n",
        "Once we find the best hyperparameters, we can train the model using those values.\n",
        "\n",
        "best_clf = grid_search.best_estimator_\n",
        "best_clf.fit(X_train, y_train)\n",
        "\n",
        "5. Evaluate the Model's Performance\n",
        "\n",
        "After training the model, we need to evaluate its performance using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC. In this healthcare context, precision and recall are particularly important.\n",
        "\n",
        "Step 1: Predictions and Accuracy\n",
        "\n",
        "Make predictions on the test set and calculate accuracy.\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred = best_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "Step 2: Precision, Recall, and F1-Score\n",
        "\n",
        "Since we may be dealing with an imbalanced dataset (more healthy patients than diseased patients), we calculate precision and recall to understand the model's performance.\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "Step 3: Confusion Matrix\n",
        "\n",
        "This will give us an idea of how many true positives, true negatives, false positives, and false negatives the model has.\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "6. Business Value of the Model\n",
        "\n",
        "This Decision Tree model can provide significant business value in the healthcare setting:\n",
        "\n",
        "Early Detection and Diagnosis: By predicting whether a patient is likely to have a disease, healthcare providers can intervene early, improving patient outcomes. This early intervention can save lives and reduce healthcare costs associated with advanced stages of the disease.\n",
        "\n",
        "Optimized Resource Allocation: Hospitals and clinics can prioritize resources, such as diagnostic tests, medical staff, and treatment, for high-risk patients. This leads to better resource management and more efficient healthcare delivery.\n",
        "\n",
        "Personalized Medicine: With the model's predictions, doctors can create more personalized treatment plans based on the patient's risk profile, leading to better treatment outcomes.\n",
        "\n",
        "Cost Savings: By accurately predicting disease risk, healthcare systems can potentially reduce costs related to misdiagnoses, unnecessary treatments, and late-stage disease interventions.\n",
        "\n",
        "Population Health Management: The model can help public health organizations identify at-risk populations, enabling more targeted public health initiatives."
      ],
      "metadata": {
        "id": "0bznK4RMtGnO"
      }
    }
  ]
}